{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba09fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are also called ** Features ** that we have to take in consideration for future prediction\n",
    "# So-> We pass these features to Our Model in order to help us Predict Label In this Case ** Class Column **\n",
    "\n",
    "# Since we know the True Label -> What it should be Called -> Thats why it'll be ** Supervised Learning **\n",
    "\n",
    "# So-> If Labeled Data (Structured Data) we do Supervised Learning.\n",
    "# For UnStructured Data -> UnLabeled Data -> We Do Unsupervised Learning.\n",
    "# -> In Unsupervised Learning we generate a Clusture of (Like - Similar) Data .\n",
    "\n",
    "# **** Summary ****\n",
    "# Supervided Learning -> Uses Labeled Inputs (input has corresponding output lables) To train models and learn outputs.\n",
    "# --> Teaching how to get to some output with similar input\n",
    "\n",
    "# Unsupervised Learning -> Uses Un-Labeled to learn about patterns in Data -> To form Clusture as output\n",
    "\n",
    "# Reinforcement Learning -> Agent Learning in interactive environment based on reward and penalties\n",
    "# -> Train our Dog -> Giving Reward for correct action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0abda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features ->\n",
    "# Qualitative Feature -> Categorical Data (Finite number of categories or groups) -> Gender, Country ... \n",
    "# No inheritant order -> Nominal Data\n",
    "\n",
    "# ONE-HOT-ENCODING\n",
    "# if it matches -> 1    Otherwise -> 0\n",
    "\n",
    "# Ordinal Data \n",
    "# Inheritant Order -> Rating 1-10 -> 1 is closer to 2 - 2 is closer to 3 .. So on.  ** But 1 is far different for 10\n",
    "# Here data is close and can be interchanged. No Strict Boundary \n",
    "# So we can mark These Kind of Data from 1 to 5 or any order or number\n",
    "\n",
    "# Quantitative Feature -> Numerical Value(Piece) of Data (Descrete or Continuous) (blue,green,yellow(Easter Egg) -- 1,2,3,4,5(Length))\n",
    "\n",
    "# So we Feed Feature (Feature Vector) to our Computer (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Supervised Learning Task ################\n",
    "# 1- Classification -> Predict Descrete Classes\n",
    "#    Binary Classification -> Your are a Student or Not a Student -> Only Two classes\n",
    "#    Multiclass Classification  -> Descrete class -> More than 2 Classes.\n",
    "\n",
    "# Ex-> Binary Classification -> Positive / Negative     ->   Cat / Dog     ->     Spam / Not Spam      (2 Thing)\n",
    "# Multiclass Classification  -> Cat / Dog / Lizard / Dolphin     -.   Orange / Apple / Pear    ->  Plant Species\n",
    "# -> Notice we have multiple classes but they kind of belong to similar **** Group ****\n",
    "\n",
    "\n",
    "# 2 -> Regression -> Predict Continuous Value\n",
    "# Here we try to predict Number close to the ** True ** data / Value -> as much as possible ** Using Diff Feature of our Dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ab69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** How do we Train out Model *****\n",
    " \n",
    "# We Break our dataset into 3 Different Dataset\n",
    "# -> Training Dataset -> 60%\n",
    "# -> Validation Dataset -> 20%\n",
    "# -> Testing Dataset -> 20%\n",
    "\n",
    "# How much Our output is different from actual Target Data is Called *** LOSS ***\n",
    "# Then make adjustment -> Training\n",
    "\n",
    "# Validation set is used for reality check\n",
    "# Loss -> Difference is not feed back to model -> instead used as reference to reduce it further\n",
    "\n",
    "# Ex ->\n",
    "# Model A  -> Loss = 1.3\n",
    "# Model B  -> Loss = 1.5\n",
    "# Model C  -> Loss = 0.5\n",
    "\n",
    "# Here model C is final as it has least loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    ************  LOSS    ****************\n",
    "\n",
    "# Loss is Difference between Prediction and Actual Label\n",
    "\n",
    "# Example of Loss Function\n",
    "# Loss (L1) = sum(|Y-Real  -  Y-Predicted|)   -> Linear (Further On X --> Greater the Loss [Y])\n",
    "# Loss (L2) = sum((Y-Real - Y-Predicted)^2)   -> Quadratic (Close to 0 -> Least loss.  Far from 0 -> Huge Loss)\n",
    "# Loss (L3) = -1/N*sum(y_Real*log(y_Predicted) + (1-y_Real)*log(1-y_Predicted)) -> Binary Cross-Entropy Loss\n",
    "#                     -> Loss Decreases as the Performance gets better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************* Accuracy **************\n",
    "\n",
    "# -> How Good Our Model has predicted \n",
    "# Calculated By *** Precision *** , *** Recall ***, *** F1 Score ***\n",
    "\n",
    "# FP(Should have been Wrong)(**False Alarm**) and FN (Should have been Correct)(**Missed Cases**) -> Wrong\n",
    "# TP and TN -> Correct\n",
    "\n",
    "# TP -> True Positive , FP -> False Positive\n",
    "# Precision -> TP / TP+FP    ->    How Many Retrived Item are Relevent\n",
    "\n",
    "# FN -> False Negative\n",
    "# Recall -> TP / TP+FN       ->    How Many relevant Items are retrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************   K - Nearest Neighbour ****************\n",
    "\n",
    "# k-> How many neighbour we use in order to judge the label (3 - 5 whatever)\n",
    "# We Find Euclidean Distance\n",
    "# Then we predict probability of its label (According to neighbours label)\n",
    "\n",
    "# we expand our distance Function in case of Higher feature (Higher Dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f010164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************** Naive Bayes *******************\n",
    "\n",
    "# Probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
